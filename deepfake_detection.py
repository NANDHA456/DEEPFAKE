# -*- coding: utf-8 -*-
"""Deepfake detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FnOcgJBYCcbw-jp2fgrs2S5ssTpT9T3C
"""

# Step 1: Install & Import Libraries
!pip install mtcnn
!pip install efficientnet
import os, json, cv2, shutil, random
import numpy as np
import pandas as pd
from mtcnn import MTCNN
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import EfficientNetB0
from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping

# Step 2: Initialize Dataset Path
DATASET_PATH = '/content/drive/MyDrive/deepfake_dataset'
VIDEO_PATH = os.path.join(DATASET_PATH, 'videos')
FRAME_PATH = os.path.join(DATASET_PATH, 'frames')
CROPPED_PATH = os.path.join(DATASET_PATH, 'faces')
METADATA_FILE = os.path.join(DATASET_PATH, 'metadata.json')

os.makedirs(FRAME_PATH, exist_ok=True)
os.makedirs(CROPPED_PATH, exist_ok=True)

# Step 3: Extract Frames from Videos
def extract_frames(video_dir, frame_dir):
    metadata = json.load(open(METADATA_FILE))
    for video_name, info in metadata.items():
        label = info['label']
        video_file = os.path.join(video_dir, video_name)
        label_dir = os.path.join(frame_dir, label)
        os.makedirs(label_dir, exist_ok=True)

        cap = cv2.VideoCapture(video_file)
        frame_rate = int(cap.get(cv2.CAP_PROP_FPS))
        frame_id = 0
        saved = 0
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break
            if frame_id % frame_rate == 0:
                resized = cv2.resize(frame, (640, 360))
                filename = f"{video_name}_{frame_id}.jpg"
                cv2.imwrite(os.path.join(label_dir, filename), resized)
                saved += 1
            frame_id += 1
        cap.release()
        print(f"Extracted {saved} frames from {video_name}")

extract_frames(VIDEO_PATH, FRAME_PATH)

# Step 4: Crop Faces using MTCNN
def crop_faces(input_path, output_path):
    detector = MTCNN()
    for label in ['REAL', 'FAKE']:
        src_dir = os.path.join(input_path, label)
        dst_dir = os.path.join(output_path, label)
        os.makedirs(dst_dir, exist_ok=True)
        for img_name in os.listdir(src_dir):
            img_path = os.path.join(src_dir, img_name)
            img = cv2.imread(img_path)
            if img is None:
                continue
            results = detector.detect_faces(img)
            if results:
                x, y, w, h = results[0]['box']
                face = img[y:y+h, x:x+w]
                face = cv2.resize(face, (128, 128))
                cv2.imwrite(os.path.join(dst_dir, img_name), face)

crop_faces(FRAME_PATH, CROPPED_PATH)

# Step 5: Split Dataset
def prepare_split(data_path):
    real_images = [os.path.join(data_path, 'REAL', f) for f in os.listdir(os.path.join(data_path, 'REAL'))]
    fake_images = [os.path.join(data_path, 'FAKE', f) for f in os.listdir(os.path.join(data_path, 'FAKE'))]

    all_images = real_images + fake_images
    labels = [0] * len(real_images) + [1] * len(fake_images)

    trainX, testX, trainY, testY = train_test_split(all_images, labels, test_size=0.2, random_state=42)

    def copy_images(img_list, label_list, base_path):
        for split, img_paths, lbls in zip(['train', 'test'], [trainX, testX], [trainY, testY]):
            for path, label in zip(img_paths, lbls):
                dst_dir = os.path.join(base_path, split, 'FAKE' if label else 'REAL')
                os.makedirs(dst_dir, exist_ok=True)
                shutil.copy(path, os.path.join(dst_dir, os.path.basename(path)))

    split_path = os.path.join(data_path, 'split')
    copy_images(all_images, labels, split_path)
    return os.path.join(data_path, 'split')

split_path = prepare_split(CROPPED_PATH)

# Step 6: Train Model (EfficientNet)
def train_model(data_path):
    BATCH_SIZE = 16
    IMG_SIZE = (128, 128)

    datagen = ImageDataGenerator(rescale=1./255)

    train_gen = datagen.flow_from_directory(os.path.join(data_path, 'train'), target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode='binary')
    test_gen = datagen.flow_from_directory(os.path.join(data_path, 'test'), target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode='binary')

    base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(128, 128, 3))
    x = GlobalAveragePooling2D()(base_model.output)
    x = Dropout(0.4)(x)
    output = Dense(1, activation='sigmoid')(x)
    model = Model(inputs=base_model.input, outputs=output)

    model.compile(optimizer=Adam(1e-4), loss='binary_crossentropy', metrics=['accuracy'])
    early_stop = EarlyStopping(patience=3, restore_best_weights=True)

    model.fit(train_gen, validation_data=test_gen, epochs=10, callbacks=[early_stop])
    return model

model = train_model(split_path)

# Step 7: Predict on a Single Image
def predict_image(image_path, model):
    img = cv2.imread(image_path)
    img = cv2.resize(img, (128, 128))
    img = img.astype('float32') / 255.0
    img = np.expand_dims(img, axis=0)
    prediction = model.predict(img)[0][0]
    print("Prediction Score:", prediction)
    print("Label:", "FAKE" if prediction > 0.6 else "REAL")

# Example
predict_image('/content/drive/MyDrive/deepfake_dataset/test_img/Copy of abarnvbtwb.mp4_87.jpg', model)